{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkLab2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVryd47Toj3B"
      },
      "source": [
        "# PySpark Lab2\n",
        "\n",
        "In this lab we will see and test some more functionality of Spark.\n",
        "\n",
        "As in the previous lab, we start the notebook by installing pyspark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_t7OxPVkNtS"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67mX47PSp59i"
      },
      "source": [
        "## Get the dataset\n",
        "\n",
        "In order to have a fast way to get the dataset we have prepared for this lab, we created a link to a file containing it in another google account, and written down all the necessary steps to get the file in the current path.\n",
        "\n",
        "This file is 2007.csv, and contains information about flights during the year 2007.\n",
        "\n",
        "Now, execute the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQaMx5sxvIto"
      },
      "source": [
        "!gdown --id \"1QJ-wDWTc3oM_jbSlb5cB3HPJ7Jwy5iAH\"\n",
        "!unzip SparkTutorials2i3.zip\n",
        "!mv Spark_Tutorial2/2006.csv Spark_Tutorial2/2007.csv Spark_Tutorial2/2008.csv .\n",
        "!rm -r __MACOSX/ Spark_Tutorial3/ Spark_Tutorial2 SparkTutorials2i3.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwY3OIb-5TfR"
      },
      "source": [
        "## Map Reduce in Spark\n",
        "\n",
        "For this lab we will be using three files. Let's load them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUcg6USv37DY"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName('testSparkSession').getOrCreate()\n",
        "\n",
        "df2006 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2006.csv\")\n",
        "df2007 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2007.csv\")\n",
        "df2008 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2008.csv\")\n",
        "\n",
        "print (\"df2006 number of partitions\", df2006.rdd.getNumPartitions())\n",
        "print (\"df2007 number of partitions\", df2007.rdd.getNumPartitions())\n",
        "print (\"df2008 number of partitions\", df2008.rdd.getNumPartitions())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74y4TJdVGZMM"
      },
      "source": [
        "Now we loaded the three files and checked the number of partitions for each of them.\n",
        "\n",
        "Let's check the number of elements of each dataframe too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3ixQKnGYC5"
      },
      "source": [
        "print (\"df2006 number of elements\", df2006.count())\n",
        "print (\"df2007 number of elements\", df2007.count())\n",
        "print (\"df2008 number of elements\", df2008.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-r6hwUKG8S1"
      },
      "source": [
        "Let's now unify all data frames into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji3M8IL0G_HM"
      },
      "source": [
        "df1 = df2006.union(df2007).union(df2008)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P5P4JDoRlKs"
      },
      "source": [
        "How many elements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsbvuhiNRlWt"
      },
      "source": [
        "df1.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOL8lTspRltt"
      },
      "source": [
        "How many partitions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzug6JXIRl-J"
      },
      "source": [
        "df1.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYKBLZBwHDPe"
      },
      "source": [
        "Let's now do some filterting.\n",
        "\n",
        "First we pick some columns, and remove the na values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzn5Wp_HDiG"
      },
      "source": [
        "df2 = df1.select(\"Year\", \"Month\", \"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
        "df3 = df2.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJuH-Mh4IVgs"
      },
      "source": [
        "Now, as in the other lab we compute the sum of arrival and departure delays, and store it in a new column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJjZ3NuDIWDj"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwviX7jIYzE"
      },
      "source": [
        "Again, we will use the cache functionality, to execute faster from this point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38dU-IP0IZCx"
      },
      "source": [
        "df4.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BSWcwi3IbZd"
      },
      "source": [
        "Let's use grouping operations, to get for instance the averafe SumDelay for each Origin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIGYMXQIb64"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "df5 = df4.groupBy(\"Origin\").agg(avg(\"SumDelay\"))\n",
        "df5.show()\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_xRayVxZQda"
      },
      "source": [
        "We can also rename a column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIhRM75_ZNuZ"
      },
      "source": [
        "df6 = df5.withColumnRenamed(\"avg(SumDelay)\", \"Average Delay\")\n",
        "df6.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}