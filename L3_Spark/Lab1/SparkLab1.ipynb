{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkLab1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVryd47Toj3B"
      },
      "source": [
        "# PySpark from Google colab\n",
        "\n",
        "In this lab we will see and test the basic functionality of Spark, and how to upload dataset files in Google colab.\n",
        "\n",
        "As you will see, installing pyspark is straightforward, so you should be easily able to install it on any computer at your disposal, with the command: pip install pyspark.\n",
        "\n",
        "Now we start the notebook by installing pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_t7OxPVkNtS"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67mX47PSp59i"
      },
      "source": [
        "## Get the dataset\n",
        "\n",
        "In order to have a fast way to get the dataset we have prepared for this lab, we created a link to a file containing it in another google account, and written down all the necessary steps to get the file in the current path.\n",
        "\n",
        "This file is 2007.csv, and contains information about flights during the year 2007.\n",
        "\n",
        "Now, execute the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQaMx5sxvIto"
      },
      "source": [
        "!gdown --id \"13yfm1bNdMBSaNp896pzrg4oCVd6JrVeP\"\n",
        "!unzip Spark_Tutorial1.zip\n",
        "!bzip2 -d Spark_Tutorial1/2007.csv.bz2\n",
        "!mv Spark_Tutorial1/2007.csv .\n",
        "!rm -r __MACOSX\n",
        "!rm Spark_Tutorial1.zip\n",
        "!rm -r Spark_Tutorial1\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwY3OIb-5TfR"
      },
      "source": [
        "## Reading the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUcg6USv37DY"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName('testSparkSession').getOrCreate()\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2007.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74y4TJdVGZMM"
      },
      "source": [
        "Now we loaded the entire file into a DataFrame named \"df\".\n",
        "\n",
        "Next we will ask Spark to print the associated schema for the data. This means, the rows of the table and the type of the data contained on each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3ixQKnGYC5"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-r6hwUKG8S1"
      },
      "source": [
        "We can also ask Spark to tell us how many partitions has it made. Depending on the number of CPU cores of your system, this number will change.\n",
        "\n",
        "The idea to make partitions in a single computer, is to allow each CPU core to process different data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji3M8IL0G_HM"
      },
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYKBLZBwHDPe"
      },
      "source": [
        "## Basic operations\n",
        "\n",
        "### Remove columns\n",
        "\n",
        "Spark offers a simple transformation to do that. Remember that transformations are not inmediatly executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzn5Wp_HDiG"
      },
      "source": [
        "df2 = df.drop(\"FlightNum\",\"TailNum\",\"UniqueCarrier\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJuH-Mh4IVgs"
      },
      "source": [
        "We can also do it in another way. Instead of removing columns, we can create a new dataframe, with only the columns we want to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJjZ3NuDIWDj"
      },
      "source": [
        "df2 = df.select(\"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwviX7jIYzE"
      },
      "source": [
        "From now on, we will work with df2 contents, but before we want to remove all the entries with a NA value in the columns ArrDelay or DepDelay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38dU-IP0IZCx"
      },
      "source": [
        "df3 = df2.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BSWcwi3IbZd"
      },
      "source": [
        "### Add columns\n",
        "\n",
        "We can add new columns and new information. For instance, in the dataframe we have information about the departure delay (DepDelay) and the arrival delay (ArrDelay). With that, we can create a new column composed by the addition of the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIGYMXQIb64"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU8ZGBdxRUeg"
      },
      "source": [
        "Now we can check the results by executing an action, which will tirgger the execution of all the accumulated transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmXv0kt0RVBh"
      },
      "source": [
        "df4.select(\"DepDelay\", \"ArrDelay\", \"SumDelay\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApOTldT8RXvw"
      },
      "source": [
        "See that the action was executed very quickly! That is because the transformations where only applied to the first 10 entries of our DataFrame.\n",
        "\n",
        "In case we are interested in knowing the maximum and minimum delays, it will take more time, because we need to traverse the entire DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uClAa2NERYbK"
      },
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "df4.select(max(\"SumDelay\"),min(\"SumDelay\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPTRmw2iRcv_"
      },
      "source": [
        "And we can also find the mean:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GnXWt2eRdCv"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "df4.select(max(\"SumDelay\"),min(\"SumDelay\"),avg(\"SumDelay\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkg-WCqRkut"
      },
      "source": [
        "### Storing intermadiate results\n",
        "\n",
        "It is not mandatory to always execute all the transformations all the time. We can ask Spark to keep the results of all the transformations already present in a data frame. That way, the next transformations we add will continue from there.\n",
        "\n",
        "The intermediate results will be stored either on RAM or hard disk. This will be decided by Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yxV_XEkRlF_"
      },
      "source": [
        "df4.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97HVnkNWTj92"
      },
      "source": [
        "### Filter operations\n",
        "\n",
        "Filter operations allow us to create new DataFrames that satisfy a condition over the data in another DataFrame.\n",
        "\n",
        "There are two transformations that allow to do that: \"where\" and \"filter\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jimmhULTneM"
      },
      "source": [
        "df5 = df4.where(\"SumDelay < 0\")\n",
        "df5.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHVD6VdoTq1B"
      },
      "source": [
        "Now we can check how many flights land before time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCZ1q906TsQ5"
      },
      "source": [
        "df3.count()\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VYaNHy4Tx-e"
      },
      "source": [
        "We can also apply more than one filter, one after the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ag6yiFVTyMf"
      },
      "source": [
        "df5 = df4.where(\"SumDelay < 0\").where(\"Origin == 'JFK'\")\n",
        "df5.show()\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPYbxempT1Qj"
      },
      "source": [
        "The \"filter\" transformation is very similar to \"where\".\n",
        "\n",
        "We can do the same operation whe previously did, but using \"filter\" instead of \"where\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH-6bRuxT1hV"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "i = 0\n",
        "city = \"JFK\"\n",
        "df5 = df4.filter(col(\"SumDelay\") < i).filter(col(\"Origin\") == city)\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBgxWtsmT5EK"
      },
      "source": [
        "#### Test\n",
        "\n",
        "Could you find the total, maximum, minimum and average delay for flights that start from 'JFK' airport? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w8dmfvV6kb"
      },
      "source": [
        "from pyspark.sql.functions import sum\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJWQ__OWAWk"
      },
      "source": [
        "## Sort operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGVDR24tT5e7"
      },
      "source": [
        "from pyspark.sql.functions import asc, desc\n",
        "df5 = df4.sort(asc(\"SumDelay\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CyqhsHrWLOa"
      },
      "source": [
        "Here we sorted the data by the total delay.\n",
        "\n",
        "This is an ascendent ordering, from minor to major.\n",
        "\n",
        "Again, this is a transformation, and is not executed until we execute an action like \"show\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZElCcIbWJxF"
      },
      "source": [
        "df5.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZeN7aFgWNpv"
      },
      "source": [
        "We can also do the ordering in descendent order. But in this case we will ask only the first 5 elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMF9YhjwWOD9"
      },
      "source": [
        "df5 = df4.sort(desc(\"SumDelay\")).limit(5)\n",
        "df5.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ht-1Id9WRel"
      },
      "source": [
        "### Obtaining unique elements\n",
        "\n",
        "How many different airports are in the DataFrame?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smWcRQKRWR-F"
      },
      "source": [
        "df5 = df4.select(\"Origin\").distinct()\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xawBziaJXUKs"
      },
      "source": [
        "#### Test\n",
        "\n",
        "How many destinations are there?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nYUwX39X0sj"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSoKh3cXmnk"
      },
      "source": [
        "Additionally, what does the following operation do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhgyQyoMXmDH"
      },
      "source": [
        "df4.select(\"Origin\",\"Dest\").distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJraRJkuX7HN"
      },
      "source": [
        "### Accessing data from python\n",
        "\n",
        "We will show how to dump df4 into a python variable and how to read it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMSWCbHMYHvz"
      },
      "source": [
        "dades = df4.limit(5).collect()\n",
        "dades\n",
        "dades[0]\n",
        "dades[0][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBTTsjS1YMlH"
      },
      "source": [
        "## Writing into files\n",
        "\n",
        "We have seen how to read a csv file from disc. Now we will see how to do the opposite.\n",
        "\n",
        "Let's save df4 into disk. Remember it has n partitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLlKWPH4YgXr"
      },
      "source": [
        "df4.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO40doMSYjAl"
      },
      "source": [
        "By default if we store df4 into a file, we would not obtain a single file, but as many files as partitions are there.\n",
        "\n",
        "To have all the data in a single frame we can use \"coalesce\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1flgLBY1Ii"
      },
      "source": [
        "df4_one = df4.coalesce(1)\n",
        "df4_one.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhtf0EDDY41X"
      },
      "source": [
        "df4_one has all the data in a single partition\n",
        "\n",
        "Now we can save it to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP1fWnzQY-Jy"
      },
      "source": [
        "df4_one.write.csv('df4_one.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldyq3XFeZUMv"
      },
      "source": [
        "### Test\n",
        "\n",
        "What happens if you try to sort the elements in df4_one? Will it be slower?\n",
        "\n",
        "Does the number of CPU cores affect the result?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yefQvdplZh6T"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}